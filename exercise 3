import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from collections import Counter

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.stem import WordNetLemmatizer

with open('moby_dick.txt', 'r', encoding='utf-8') as file:
    moby_dick_text = file.read()

tokens = word_tokenize(moby_dick_text.lower())  # Convert to lowercase for consistency

stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

pos_tags = pos_tag(filtered_tokens)

pos_counts = Counter(tag for word, tag in pos_tags)
top_pos = pos_counts.most_common(5)

lemmatizer = WordNetLemmatizer()
top_lemmas = [lemmatizer.lemmatize(word) for word, _ in pos_tags[:20]]

pos_labels, pos_freqs = zip(*pos_counts.items())

plt.figure(figsize=(12, 6))
plt.bar(pos_labels, pos_freqs)
plt.xlabel('Parts of Speech')
plt.ylabel('Frequency')
plt.title('POS Frequency Distribution in Moby Dick')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print("Top 5 Parts of Speech and their Frequencies:")
for pos, count in top_pos:
    print(f"{pos}: {count}")

print("\nTop 20 Lemmatized Tokens:")
for lemma in top_lemmas:
    print(lemma)
